model = google/t5-v1_1-large, sharded with 200000 parameters


--> google/t5-v1_1-large has 195787520 params


Model arch: 
FullyShardedDataParallel(
  (_fsdp_wrapped_module): FlattenParamsWrapper(
    (_fpw_module): T5ForConditionalGeneration(
      (shared): FullyShardedDataParallel(
        (_fsdp_wrapped_module): FlattenParamsWrapper(
          (_fpw_module): Embedding(32128, 1024)
        )
      )
      (encoder): T5Stack(
        (embed_tokens): Embedding(32128, 1024)
        (block): ModuleList(
          (0): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (relative_attention_bias): Embedding(32, 16)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (2): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (3): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (4): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (5): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (6): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (7): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (8): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (9): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (10): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (11): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (12): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (13): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (14): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (15): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (16): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (17): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (18): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (19): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (20): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (21): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (22): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (23): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (final_layer_norm): T5LayerNorm()
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (decoder): T5Stack(
        (embed_tokens): Embedding(32128, 1024)
        (block): ModuleList(
          (0): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (relative_attention_bias): Embedding(32, 16)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (2): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (3): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (4): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (5): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (6): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (7): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (8): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (9): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (10): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (11): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (12): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (13): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (14): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (15): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (16): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (17): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (18): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (19): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (20): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (21): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (22): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (23): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (final_layer_norm): T5LayerNorm()
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (lm_head): FullyShardedDataParallel(
        (_fsdp_wrapped_module): FlattenParamsWrapper(
          (_fpw_module): Linear(in_features=1024, out_features=32128, bias=False)
        )
      )
    )
  )
)
model wrapping = 
FullyShardedDataParallel(
  (_fsdp_wrapped_module): FlattenParamsWrapper(
    (_fpw_module): T5ForConditionalGeneration(
      (shared): FullyShardedDataParallel(
        (_fsdp_wrapped_module): FlattenParamsWrapper(
          (_fpw_module): Embedding(32128, 1024)
        )
      )
      (encoder): T5Stack(
        (embed_tokens): Embedding(32128, 1024)
        (block): ModuleList(
          (0): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (relative_attention_bias): Embedding(32, 16)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (2): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (3): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (4): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (5): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (6): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (7): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (8): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (9): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (10): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (11): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (12): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (13): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (14): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (15): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (16): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (17): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (18): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (19): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (20): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (21): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (22): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (23): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (final_layer_norm): T5LayerNorm()
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (decoder): T5Stack(
        (embed_tokens): Embedding(32128, 1024)
        (block): ModuleList(
          (0): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (relative_attention_bias): Embedding(32, 16)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (2): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (3): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (4): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (5): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (6): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (7): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (8): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (9): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (10): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (11): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (12): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (13): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (14): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (15): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (16): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (17): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (18): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (19): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (20): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (21): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (22): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (23): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (k): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (v): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                  (o): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=1024, bias=False)
                    )
                  )
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wi_1): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=1024, out_features=2816, bias=False)
                    )
                  )
                  (wo): FullyShardedDataParallel(
                    (_fsdp_wrapped_module): FlattenParamsWrapper(
                      (_fpw_module): Linear(in_features=2816, out_features=1024, bias=False)
                    )
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (final_layer_norm): T5LayerNorm()
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (lm_head): FullyShardedDataParallel(
        (_fsdp_wrapped_module): FlattenParamsWrapper(
          (_fpw_module): Linear(in_features=1024, out_features=32128, bias=False)
        )
      )
    )
  )
)


